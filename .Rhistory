library(datasets)
?data
data(iris)
?iris
iris
load("C:/Users/Conor/git/Coursera/Data_specialization/UCI HAR Dataset/snapshot20140726.RData")
rm(iris)
gc()
install.packages("KernSmooth")
library(KernSmooth)
load("C:/Users/Conor/git/pubmed/pubmed_macrocycle_query.R.RData")
write.csv(query.unique.name[1001:1201,], filename = new_sample1.csv )
?write.csv
write.csv(query.unique.name[1001:1201,], file = new_sample1.csv )
write.csv(query.unique.name[1001:1201,], "file = new_sample1.csv" )
str(query.unique.name)
write.csv(query.unique.name[4001:4201,], file = "new_sample2.csv" )
write.csv(query.unique.name[6001:6201,], file = "new_sample3.csv" )
install.packages("RMySQL")
install.packages("RMySQL", type=source)
install.packages("RMySQL", type="source")
library(RMySQL)
install.packages("RMySQL", type="source")
install.packages("RMySQL", type="source")
install.packages("RMySQL", type="source")
load("C:/Users/Conor/git/pubmed/pubmed_macrocycle_query.R.RData")
source('C:/Users/Conor/git/pubmed/002_pubmed_peptide_groups_by_country_01.R')
library(Countrycode)
install.package("Countrycode")
install.packages("Countrycode")
install.packages("countrycode")
source('C:/Users/Conor/git/pubmed/002_pubmed_peptide_groups_by_country_01.R')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
rm(sam*)
rm(sample1)
rm(sample2)
rm(sample3)
rm(sample4)
rm(sample5)
rm(sample6)
query.affiliation
getwd()
source('C:/Users/Conor/git/pubmed/002_pubmed_peptide_groups_by_country_01.R')
source('~/.active-rstudio-document')
source('C:/Users/Conor/git/pubmed/002_pubmed_peptide_groups_by_country_01.R')
n <- 150
p <- 2
sigma <- 1
meanpos <- 0
meanneg <- 3
npos <- round(n/2)
nneg <- npos
xpos <- matrix(rnorm(npos*p, mean=meanpos, sd=sigma),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanneg, sd=sigma),npos, p)
x <- rbind(xpos,xneg)
View(x)
View(xneg)
View(xpos)
y <- matrix(c(rep(1,npos),rep(-1,neg)))
y <- matrix(c(rep(1,npos),rep(-1,nneg)))
plot(x,col=ifelse(y>0,1,2))
legend("topleft",c('Positive','Negative'), col=seq(2),pch=1,text.col=seq(2))
ntrain <- round(n*0.8)
tindex <- sample(n,ntrain
)
discard1 <- sample(n,20)
del(discard1)
rm(discard1)
xtrain <- x[tindex,]
ytrain <- y[tindex]
xtrain <- x[-tindex,]
ytest <- y[-tindex]
istrin=rep(0,n)
istrain[tindex]=1
istrain=rep(0,n)
rm(istrin)
istrain[tindex]=1
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
legend("topleft",c('Positive Train','Positive Test','Negative Train','Negative Test'),col=c(1,1,2,2),pch=c(1,2,1,2),text.col=c(1,1,2,2))
install.packages('kernlab')
library(kernlab)
svp  <- ksvm(xtrain,ytrain,type="C-svc",kernal='vanilladot',C=100,scaled=c())
svp
attributes(svp)
View(xtrain)
alpha(svp)
alphaindex(svp)
b(svp)
plot(svp,data=xtrain)
rm *
rm
rm(*)
rm all
rm(list = ls())
install.packages('e1071')
library(e1071)
library(MASS)
data(cats)
model <- svm(Sex~.,data = cats)
View(cats)
print(model)
summary(model)
plot(model,cats)
help(predict.svm)
index <- 1:nrow(cats)
testindex <- sample(index,trunc(length(index)/3))
testset <- cats[testindex]
testset <- cats[testindex,]
trainset <- cats[-testindex,]
model <- svm(Sex~., data=trainset)
prediction <- predict(model,testset[,-1])
tab <- table(pred = prediction, true = testset[,-1])
tab <- table(pred = prediction, true = testset[,1])
tab
classAgreement(tab)
tuned <- tune.svm(Sex~., data=trainset,gamma-10^(-6:-1), cost = 10^(1:2))
summary(tuned)
tuned <- tune.svm(Sex~., data=trainset,gamma=10^(-6:-1), cost = 10^(1:2))
summary(tuned)
dataset <- read.csv('C:Users/Conor/wdbc.data', head = FALSE)
dataset <- read.csv('C:/Users/Conor/wdbc.data', head = FALSE)
rm(list=ls())
dataset <- read.csv('C:/Users/Conor/wdbc.data', head = FALSE)
index <- 1:nrow(dataset)
testindex <- sample(index,trunc(length(index)*0.3)))
testindex <- sample(index,trunc(length(index)*0.3))
testset <- dataset[testindex,]
trainset <- dataset[-testindex,]
-testindex
dataset[-1]
names(dataset)
tuned <- tune.svm(V2~., data=trainset, gamma=10^(-6:-1), cost=10^(-1:1))
dataset["V2"]
summary(tuned)
model <- svm(V2~., data=trainset, kernal='radial', gamma=0.001,cost=10)
summary(model)
prediction <- predict(model, testset[,-2])
tab <- table(pred = prediction, true=trueset[,2])
tab <- table(pred = prediction, true=testset[,2])
tab
dataset <- read.csv('C:\Users\Conor\wdbc.data', head=F)
dataset <- read.csv('C:/Users/Conor/wdbc.data', head=F)
index <- 1:nrow(dataset)
testindex <- sample(index, trunc(length(index)*0.3))
?trunc
length(index)*0.3
trunc(length(index)*0.3)
testset <- dataset[testindex,]
trainset <- dataset[-testindex,]
dataset[568]
dataset[568,]
dataset[-1,]
names(dataset)
tuned <- tune.svm(V2~., data=trainset, gamma=10^(-6:-1), cost=10^(-1:1))
library(e1071)
tuned <- tune.svm(V2~., data=trainset, gamma=10^(-6:-1), cost=10^(-1:1))
View(testset)
summary(tuned)
model <- svm(V2~., data=trainset, kernal="radial", gamma=0.001, cost=10)
summary(model)
prediction <- predict(model, testset[,-2])
tab <- table(pred=prediction, true=testset[,2])
tab
testset[,-2]
testset[,-20]
source('C:/Users/Conor/GDrive/Data_Analysis/20151203__R__SVM_tutorial/20151203__R_SVM__breast_cancer_diagnosis.R')
r.home()
R.home()
fileURL <- ("http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv ")
q1 <- read.csv(fileURL)
names(q1)
strsplit(names(q1), "wgtp")
fileURL <- ("http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv") #remove https, replace with http
q1 <- read.csv(fileURL)
View(`q1`)
q2 <- read.csv(fileURL)
names(q2)
q2$Gross.domestic.product.2012
q2$X3
names(q2)
q2$X.3
?mean
mean(grep(q2$X.3, "", ","))
mean(gsub(q2$X.3, "", ","))
mean(gsub("", ",", q2$X.3))
str(q2$X.3)
gdp_cut <- gdp[5:194,]
q2_cut <- q2[5:194,]
mean(gsub("", ",", q2_cut$X.3))
q2 <- read.csv(fileURL, stringsAsFactors=F)
q2_cut <- q2[5:194,]
mean(gsub("", ",", q2_cut$X.3))
q2_cut <- q2[5:194,]
str(q2_cut)
str(q2_cut$X.3)
q2_numeric = as.numeric(q2_cut$X.3)
q2_numeric  <- as.numeric(q2_cut$X.3)
<- q2_cut$X.3
q2_GDP <- q2_cut$X.3
q2_nocomma <- gsub("",",",q2_GDP)
q2_nocomma <- gsub(",","",q2_GDP)
mean(q2_nocomma)
q2_numeric <- as.numeric(q2_nocomma)
q2_mean <- mean(q2_numeric)
grep("^United",q2$countryNames)
View(`q2_cut`)
grep("^United",q2$X.2)
grepl("^United",q2$X.2)
table(grepl("^United",q2$X.2))
Q4fileURL2 <- ("http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv")
q4 <- read.csv(Q4fileURL2)
head(q4)
head(q2_cut)
q4_mergedData = merge(gdp_cut,ed,by.x="X",by.y="CountryCode")
q4_mergedData = merge(q2_cut,q4,by.x="X",by.y="CountryCode")
head(q4_mergedData)
q4_mergedData$Special.Notes
table(grepl("Fiscal year end: June"), q4_mergedData$Special.Notes)
table(grepl("Fiscal year end: June"), q4_mergedData$Special.Notes))
table(grepl("Fiscal year end: June", q4_mergedData$Special.Notes))
install.packages('quantmod')
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
View(amzn)
str(amzn$row.names)
names(amzn)
?index
sampleTimes
str(sampleTimes)
?count
library(lubridate)
install.packages('lubridate')
library(lubridate)
table(which(%Y, sampleTimes)
table(which(%Y, sampleTimes))
which(%Y, sampleTimes)
?year
year(sampleTimes[1])
which(year(sampleTimes == 2012))
which(year(as.POSIXct(sampleTimes) == 2012))
which(year(as.POSIXlt(sampleTimes) == 2012))
year(sampleTimes)
year(sampleTimes) == 2012
which(year(sampleTimes) == 2012)
table(which(year(sampleTimes) == 2012))
length(which(year(sampleTimes) == 2012))
length(which(year(sampleTimes) == 2012 & wday == "Monday"))
?wday
length(which(year(sampleTimes) == 2012 & day == "Monday"))
length(which(year(sampleTimes) == 2012 & day(sampleTimes) == "Monday"))
length(which(day(sampleTimes) == "Monday"))
length(which(day(sampleTimes) == Monday))
length(which(wday(sampleTimes) == "Monday"))
length(which(wday(sampleTimes) == "Monday", labels = T))
wday(sampleTimes)
wday(sampleTimes, label=T)
length(which(wday(sampleTimes == "Mon", label=T)))
which(wday(sampleTimes == "Mon", label=T))
which(wday(sampleTimes = "Mon", label=T))
which(wday(sampleTimes == "Mon", label=T))
which(wday(sampleTimes == "Mon"))
wday(sampleTimes == "Mon")
wday(sampleTimes == 2)
wday(sampleTimes) == 2)
wday(sampleTimes) == 2
length(which(wday(sampleTimes) == "Mon"))
length(which(wday(sampleTimes) == "Mon", labels=T))
length(which(wday(sampleTimes, labels=T) == "Mon"))
wday(sampleTimes = Mon, label=T)
wday(sampleTimes, label=T)
wday(sampleTimes == Mon, label=T)
wday(sampleTimes == "Mon", label=T)
wday(sampleTimes == "Mon")
wday(sampleTimes = "Mon")
length(which(wday(sampleTimes) ==2))
length(which(wday(sampleTimes) ==2 & year(sampleTimes) == 2012))
install.packages('knitr')
library(kernlab)
data(spam)
set.seed(3435)
trainIdicator <- rbinom(4601, size = 1, prob = 0.5)
trainIdicator <- trainIndicator
trainIndicator <- trainIdicator
rm(trainIdicator)
table(trainIndicator)
trainSpam = spam[trainIndicator] == 1,]
trainSpam = spam[trainIndicator == 1,]
testSpam  <- spam[trainIndicator == 0, ]
names(spam)
head(trainSpam)
table(trainSpam$tyoe)
table(trainSpam$type)
plot(log(trainSpam$capitalAve + 1) ~ trainSpam$type)
plot(log(trainSpam[,1:4]+1))
hclust <- hclust(dist(t(trainSpam[, 1:57])))
plot(hclust)
hClusterUpdated <- hclust(dist(t(log(trainSpam[, 1:55] + 1))))
plot hClusterUpdated
hCluster <- hclust(dist(t(trainSpam[, 1:57])))
hClusterUpdated <- hclust(dist(t(log(trainSpam[, 1:55] + 1))))
plot hClusterUpdated
plot hClusterUpdated
plot(hClusterUpdated)
trainSpam$numType <- as.numeric(trainSpan$type) -1
trainSpam$numType <- as.numeric(trainSpam$type) -1
costFunction <- function(x,y) sum(x!=(y > 0.5))
cvError <- rep(NA, 55)
library(boot)
for (i in 1:55){}
for (i in 1:55){
lmFormula <- reformulate(names(trainSpam)[i], response = 'numType')
glmFit <- glm(lmFormula, family = 'binomial', data = trainSpam)
cvError[i] <- cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}
names(trainSpam)[which.min(cvError)]
install.packages('rmarkdown')
library(rmarkdown)
predictionModel <- glm(numType ~ charDollar, family = 'binomial', data = trainSpam)
predictionModel <- predict(predictionModel, testSpam)
predictedSpam <- rep('nonspam', dim(testSpam)[1])
predictedSpam[predictionModel$fitted > 0.5] = 'spam'
table(predictedSpam, testSpam$type)
rm(list = ls())
setwd('C:/Users//Conor/GDrive/GD--Research/20160113__encycle__CyclophilinD_ligand_design/')
top1000 <- read.csv('./output//20160113__1152__docking_results_table_1000.csv')
top1000arom <- read.csv('./output//20160113__1152__docking_results_table_1000_aromatic.csv')
View(top1000arom)
View(top1000arom)
meanAll <- mean(top1000$glide.gscore)
meanArom <- mean(top1000arom$glide.gscore)
meanArom <- mean(top1000arom$glide.gscore, na.rm=T)
medAll <- median(top1000$glide.gscore)
medArom <- medAll(top1000arom$glide.gscore, na.rm=T)
medArom <- median(top1000arom$glide.gscore, na.rm=T)
View(top1000arom)
View(top1000arom)
LPheDPhe <- subset(top1000arom, (grep('L-Phe,D-Phe', top1000arom$Title)))
LPheDPhe <- top1000arom[grep('L-Phe,D-Phe', top1000arom$Title),]
meanLD <- <- mean(LPheDPhe$glide.gscore, na.rm=T)
meanLD <- mean(LPheDPhe$glide.gscore, na.rm=T)
medLD <- median(LPheDPhe$glide.gscore, na.rm=T)
par(mfrow = c(1, 2))
par(mfrow = c(1, 3))
boxplot(top1000$glide.gscore)
boxplot(top1000arom$glide.gscore)
boxplot(LPheDPhe$glide.gscore)
boxplot(top1000$glide.gscore, ylim=c(-5, -9))
boxplot(top1000arom$glide.gscore, ylim=c(-5, -9))
boxplot(LPheDPhe$glide.gscore, ylim=c(-5, -9))
par(mfrow = c(1, 3), title = "Distribution of Glide Scores across various sub-groups of docked ligands")
par(mfrow = c(1, 3), title(main= "Distribution of Glide Scores across various sub-groups of docked ligands")
par(mfrow = c(1, 3), title(main= "Distribution of Glide Scores across various sub-groups of docked ligands"))
mtext(“Distribution of Glide Scores across various sub-groups of docked ligands”, outer = TRUE, cex = 1.5)
mtext('Distribution of Glide Scores across various sub-groups of docked ligands', outer = TRUE, cex = 1.5)
boxplot(top1000$glide.gscore, ylim=c(-5.5, -8.5))
boxplot(top1000arom$glide.gscore, ylim=c(-5.5, -8.5))
boxplot(LPheDPhe$glide.gscore, ylim=c(-5.5, -8.5))
par(mfrow = c(1, 3))
mtext('Distribution of Glide Scores across various sub-groups of docked ligands', outer = TRUE, cex = 1.5)
boxplot(top1000$glide.gscore, ylim=c(-5.5, -8.5))
boxplot(top1000arom$glide.gscore, ylim=c(-5.5, -8.5))
boxplot(LPheDPhe$glide.gscore, ylim=c(-5.5, -8.5))
mtext('Distribution of Glide Scores across various sub-groups of docked ligands', outer = TRUE, cex = 1.5)
View(top1000arom)
rm(list = ls())
setwd("C:/Users/Conor/git/reproducible_research_assignment1")
dat <- read.csv('./data/activity.csv')
View(dat)
str(dat)
hist(dat$steps ~ dat%date)
hist(dat$steps, dat%date)
hist(dat$steps, dat$date)
hist(dat$steps ~dat$date)
